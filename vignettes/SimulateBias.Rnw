%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Simulate estimation after blinded interim estimations}
\documentclass{article}

\title{Simulation study of the properties of point estimates and
  confidence intervals}
\author{Florian Klinglmueller}
\usepackage{colortbl}
\usepackage{booktabs}
\newcommand{\ie}{{\em i.e.,~}}
\begin{document}

\maketitle

\section{Introduction}

This vignette recomputes the simulation study presented in Section 4
of the paper ``Estimation after blinded interim analysis''. Simply
compiling the Sweave file using \texttt{knitr} will recompute all
figures using stored simulation results. To rerun the simulations the
corresponding R chunks need to be reconfigured to be evaluated. Note
that this may take a long time. Also simulation code has been
optimized to run on Linux, so while the package should work fine on
Windows in theory, it has not been tested. 

\section{Simulation Study}

In the simulation study we reassessed the second stage sample size
using the adjusted and the unadjusted variance estimate. In each stage
we generated normally distributed test statistics for the control and
treatment group assuming balanced allocation.

For running the simulations and plotting the results the following
packages have to be loaded:

<<>>=
library(parallel)
library(blindConfidence)
library(ggplot2)
options(mc.cores=detectCores()-2)

@

The next chunk generates the matrix of parameters for which
simulations are run.

<<>>=
G <- expand.grid(delta=round(seq(-2,2,.2),2),
                 sigma=round(c(.1,.25,seq(.5,2,.5)),2),
                 d = c(.2,.6,1,2),
                 s = round(seq(.5,2,.5),2))
G$n1 <- ceiling(1/2*zss(G$s,G$d,.025,.2))
@

We assume that the pre-planned sample size of the trial was planned
for a one-sided significance level $\alpha=0.025$ assuming an effect
size $\delta=1$ and standard deviations $\hat{\sigma}$ to reach a
target power of 80% (using the normal approximation). The first stage
sample size $n_1$ was set to halve of the pre-planned sample size. The
simulations were performed in R with $10^6$ simulation runs per
scenario.

The next chunk runs the simulations. In order to rerun them it has to
be reconfigured for evaluation. Note that running the simulations may
take quite long.

<<eval=FALSE>>=
set.seed(50014056)
runs <- 10^6
gridsim <- t(simplify2array(mclapply(1:nrow(G),
                                     function(i) {c(G[i,],
                                                    simVBIA(delta=G[i,]$delta,
                                                            sigma=G[i,]$sigma,
                                                            d=G[i,]$d,
                                                            s=G[i,]$s,
                                                            cf=1,
                                                            runs=runs,
                                                            alpha=.025,
                                                            beta=.2))})))


gsim <- as.data.frame(t(apply(gridsim,1,unlist)))

## compute relative bias
gsim$var.rbias <- gsim$variance.bias/gsim$sigma^2

## compute the theoretical lower bound for the variance bias
gsim$bound <- lowerBound(gsim$n1,gsim$d)

runs <- 10^5
bsim <- mclapply(1:nrow(gsim),function(i) {c(gsim[i,],
                                             simMBIA(delta=gsim[i,]$delta,
                                                     sigma=gsim[i,]$sigma,
                                                     n1=gsim[i,]$n1,
                                                     runs=runs))})

bsim <- do.call('cbind',bsim)
bsim <- as.data.frame(apply(bsim,1,unlist),row.names=NA)

gridsim <- bsim
gridsim$brannath <- .4*sqrt(2)*gridsim$sigma/sqrt(gridsim$n1)

@

Applying the simulation function, in parallel, across rows of matrix
$G$, which contains the parameter settings for each scenario.


<<eval=FALSE,echo=FALSE>>=
fname <- paste('gridsim_',format(Sys.time(),"%y%m%d"),'.Rd',sep='')
save(gridsim,file=fname)
@

\section{Results}

The data used in the final manuscript version, is distributed with this
package. 

<<>>=
data(gridsim)

full.sim <- gridsim

gridsim <- subset(full.sim,d==1 & sigma>=.5)


@

<<results='hide',include=F,echo=F>>=
#This little function takes care of annotating the plots.
lverbose = function(variable,value){
    if(variable == "s"){
        sapply(value,function(val) substitute(paste(sigma[0]," = ",foo,", ",n[1]," = ",n1,sep=''),
                                              list(foo=val,n1=ceiling(1/2*zss(val,1,.025,.2)))))
    } else {
        sapply(value,function(val) substitute(paste(sigma," = ",foo,sep=""),list(foo=val)))
    }
}

@

Next we set up the figures using \texttt{ggplot2} objects. We plot 
how far the coverage probabilities of one and two-sided confidence
intervals deviate from their nominal coverage. 

\subsection*{Coverage probability of confidence intervals}


First we show results
for designs that reassess the sample size based on the adjusted
interim variance estimate. Negative values correspond to situations
where the actual coverage is smaller than the nominal coverage. In
this case the coresponding confidence intervals become liberal. 

\subsubsection*{Adjusted interim variance estimate}

<<results='hide',echo=F,warning=F>>=
tupd <- ggplot(gridsim,mapping=aes(x=delta,y=.025-upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + 
    geom_path(aes(y=.025-lower.prob),lty=3) + 
    geom_path(aes(y=.05-total.prob)) + 
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-.025,.01) +
    xlab(expression(delta)) + ylab('actual - nominal coverage')
print(tupd)
@

\subsubsection*{Unadjusted interim variance estimate}

Then also for designs that use the unadjusted lumped variance at
interim.

<<results='hide',echo=F,warning=F>>=

tupd.uc <- ggplot(gridsim,mapping=aes(x=delta,y=.025-uc.upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + 
    geom_path(aes(y=.025-uc.lower.prob),lty=3) + 
    geom_path(aes(y=.05-uc.total.prob)) + 
    facet_grid(sigma~s,labeller=lverbose) + theme_bw()+
    ylim(-.025,.01) + xlab(expression(delta)) + ylab('actual -nominal coverage')
print(tupd.uc)

@

\subsection*{Bias of the effect size estimate}

Next we look at estimates of the mean difference between treatment
groups. Solid lines show the mean bias for designs where sample sizes
are adjusted based on the adjusted interim variance estimate, dashed
lines for designs where sample sizes are adjusted based on the
unadjusted interim variance estimate. The dotted lines show the
bias of the mean estimate using a samplesize reassessment rule is used
in order to maximize the negative or positive bias, respectively.

<<results='hide',echo=F,warning=F>>=

tupd.mean.bias <- ggplot(gridsim,mapping=aes(x=delta,y=mean.bias)) +
    geom_path(lty=2) + geom_line(y=0,colour='gray') +
    geom_path(aes(y=m.bias),lty=3,col='red') +
    geom_path(aes(y=m.bias.n),lty=3,col='red') +
    geom_path(aes(y=uc.mean.bias),lty=1) +
    geom_path(aes(y=brannath),lty=3,col='darkgreen') +
    geom_path(aes(y=-brannath),lty=3,col='darkgreen') +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() +
    xlab(expression(delta)) + ylab('bias') + ylim(-.2,.2)
print(tupd.mean.bias)

@

\subsection*{Bias of the variance estimate}

Next we look at variance estimates. 

<<results='hide',echo=F,warning=F>>=

tupd.var.bias <- ggplot(gridsim,mapping=aes(x=delta,y=variance.bias)) +
    geom_line(y=0,colour='gray') + geom_path(aes(y=bound),color="red") +
    geom_path(lty=2) + geom_path(aes(y=uc.variance.bias),lty=1) +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-0.4,0.1) +
    xlab(expression(delta)) + ylab('bias')
print(tupd.var.bias)

@

\subsection*{Estimates of the variance of the mean estimate (squared
  standard error)}

In each simulated trial we estimated the standard error of the mean,
and compute the standard error of a fixed sample trial with the
same final sample size. Finally, we compute the standard deviation of
the mean estimate across all monte carlo samples, which is an unbiased
estimate of the true standard error of the mean. Figure ? shows the
results for designs that use the corrected interim variance
estimate to reassess the sample size; Figure ? shows the results for
designs that use the unadjusted interim variance estimate. Solid lines
show the standard deviation of mean estimates across simulated trials,
dashed lines show the average standard error estimates across
simulated trials, dotted lines show the average of the true standard
errors of corresponding fixed sample designs. The true standard error of the mean
of an adaptive design with blinded sample size reassessment is smaller
than that of corresponding fixed sample design. The bias of the
estimate of the standard error is close to zero around the null
hypothesis.  This holds both for designs using the adjusted and
unadjusted interim variance estimates to reassess the sample size. It
explains why  there is practically no inflation of the coverage
probability of confidence intervals near the null hypotheses, even
though the variance estimate has a considerable negative bias.

An explanation is given in the following figure where we plot the
standard error of the mean, averaged across all simulated trials; the
standard error of the mean of a fixed sample desig the standard
deviation of the mean estimate across all monte-carlo samples
(i.e. the true standard error of the mean) of a given parameter
setting (dashed line); the Conditional on the second stage sample size
the estimate of the standard error is smaller than the standard error
of a fixed sample design with equal sample size. However the true
variance of the mean - as estimated from all monte carlo samples - is
estimated across all simulation runs. This shows that the
standard error of a fixed sample design with average sample size is
larger than the standard error of the adaptive design.

\subsubsection*{Adjusted interim variance estimate}

<<results='hide',echo=F,warning=F>>=
tupd.vm.bias <- ggplot(gridsim,mapping=aes(x=delta,y=vm)) +geom_line(lty=1) +
    geom_path(aes(y=ev),lty=2) + geom_path(aes(y=exv),lty=3) +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() +
    xlab(expression(delta)) + ylab('Variance of the mean estimate')

print(tupd.vm.bias)
@

\subsubsection*{Unadjusted interim variance estimate}

<<results='hide',echo=F,warning=F>>=

tupd.uc.vm.bias <- ggplot(gridsim,mapping=aes(x=delta,y=uc.vm)) +
    geom_line(lty=1) + geom_path(aes(y=uc.ev),lty=2) +
    geom_path(aes(y=uc.exv),lty=3) + facet_grid(sigma~s,labeller=lverbose) +
    theme_bw() + xlab(expression(delta)) +
    ylab('Variance of the mean estimate')

print(tupd.uc.vm.bias)

@


\end{document}

<<eval=F,echo=F,include=F>>=
## this chunk produces the files that go into the paper
cairo_ps('coverage.eps')
print(tupd)
dev.off()
cairo_ps('uc_coverage.eps')
print(tupd.uc)
dev.off()
cairo_ps('varmeanBias.eps')
print(tupd.vm.bias)
dev.off()
cairo_ps('uc_varmeanBias.eps')
print(tupd.uc.vm.bias)
dev.off()
cairo_ps('mean_bias.eps')
print(tupd.mean.bias)
dev.off()
cairo_ps('var_bias.eps')
print(tupd.var.bias)
dev.off()
@
