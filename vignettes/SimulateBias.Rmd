<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Simulate estimation after blinded interim estimations}
-->

# Simulation study of the properties of point estimates and confidence intervals

In the simulation study we reassessed the second stage sample size
using the adjusted and the unadjusted variance estimate. In each stage
we generated normally distributed test statistics for the control and
treatment group assuming balanced allocation.


```{r} 
library(parallel)
library(blindConfidence)
library(ggplot2)
options(mc.cores=detectCores()-2)

```

```{r}
G <- expand.grid(delta=round(seq(-2,2,.2),2),sigma=round(c(.1,.25,seq(.5,2,.5)),2),d = c(.2,.6,1,2), s = round(seq(.5,2,.5),2))
G$n1 <- ceiling(1/2*zss(G$s,G$d,.025,.2))
```

We assume that the pre-planned sample size of the trial was planned
for a one-sided significance level $\alpha=0.025$ assuming an effect
size $\delta=1$ and standard deviations $\hat{\sigma}$ to reach a
target power of 80% (using the normal approximation). The first stage
sample size $n_1$ was set to halve of the pre-planned sample size. The
simulations were performed in R with $10^6$ simulation runs per
scenario and the code is available as supplementary material.


```{r,eval=FALSE}
set.seed(50014056)
runs <- 10^6
gridsim <- t(simplify2array(mclapply(1:nrow(G),function(i) {c(G[i,],simVBIA(delta=G[i,]$delta,sigma=G[i,]$sigma,d=G[i,]$d,s=G[i,]$s,cf=1,runs=runs,alpha=.025,beta=.2))})))


gsim <- as.data.frame(t(apply(gridsim,1,unlist)))

## compute relative bias
gsim$var.rbias <- gsim$variance.bias/gsim$sigma^2

## compute the theoretical lower bound for the variance bias
gsim$bound <- lowerBound(gsim$n1,gsim$d)

runs <- 10^5
bsim <- mclapply(1:nrow(gsim),function(i) {c(gsim[i,],simMBIA(delta=gsim[i,]$delta,sigma=gsim[i,]$sigma,n1=gsim[i,]$n1,runs=runs))})

bsim <- do.call('cbind',bsim)
bsim <- as.data.frame(apply(bsim,1,unlist),row.names=NA)

gridsim <- bsim

```
We computed the simulation results shown in the final manuscript
version using `r runs`. Applying the simulation function, in parallel,
across rows of matrix `G`, which contains the parameter settings for
each scenario.


```{r,eval=FALSE,echo=FALSE}
fname <- paste('~/gridsim_',format(Sys.time(),"%y%m%d"),'.Rd',sep='')
save(gridsim,file=fname)
```

Instead of running the simulation each time, we can just load the data
used in the final manuscript version, which is distributed with this
package. 

```{r}
data(gridsim)

full.sim <- gridsim
gridsim <- subset(full.sim,d==1)

```

```{r}

```
In fact the scenarios shown in the article comprise just a subset of
the scenarios computed in the simulation. 

Overall simulations were performed for all possible combinations
between the following values of design parameters:

```{r,echo=F,results='asis'}
cat("<dl>\n")
for(i in c('delta','sigma','d','s')){
   cat("<dt>",i,"</dt>\n","<dd>",unique(gsim[,i]),"</dd>\n")
   }
cat("</dl>")
```

This little function takes care of annotating the plots.

```{r,results='hide'}

lverbose = function(variable,value){
    if(variable == "s"){
        sapply(value,function(val) substitute(paste(sigma[0]," = ",foo,", ",n[1]," = ",n1,sep=''),
                                              list(foo=val,n1=ceiling(1/2*zss(val,1,.025,.2)))))
    } else {
        label_parsed(variable,label_both(variable,value))
    }
}

```

Next we set up the figures using `ggplot2` objects. We plot 
how far the coverage probabilities of one and two-sided confidence
intervals deviate from their nominal coverage. First we show results
for designs that reassess the sample size based on the adjusted
interim variance estimate. Negative values correspond to situations
where the actual coverage is smaller than the nominal coverage. In
this case the coresponding confidence intervals become liberal. 

```{r}

tupd <- ggplot(gridsim,mapping=aes(x=delta,y=.025-upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + #geom_point(shape='u') +
    geom_path(aes(y=.025-lower.prob),lty=3) + #geom_point(aes(y=.025-lower.prob),shape='l') +
    geom_path(aes(y=.05-total.prob)) + #geom_point(aes(y=.05-total.prob),shape='t') +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-.025,.01) +
    xlab(expression(delta)) + ylab('actual - nominal coverage')

print(tupd)

```
Then also for designs that use the unadjusted lumped variance at
interim.

```{r}

tupd.uc <- ggplot(gridsim,mapping=aes(x=delta,y=.025-uc.upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + # geom_point(shape='u') +
    geom_path(aes(y=.025-uc.lower.prob),lty=3) + #geom_point(aes(y=.025-uc.lower.prob),shape='l') +
    geom_path(aes(y=.05-uc.total.prob)) + #geom_point(aes(y=.05-uc.total.prob),shape='t') +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw()+  ylim(-.025,.01) + xlab(expression(delta)) + ylab('actual -nominal coverage')
print(tupd.uc)

```
Next we look at estimates of the mean difference between treatment
groups. Solid lines show the mean bias for designs where sample sizes
are adjusted based on the adjusted interim variance estimate, dashed
lines for designs where sample sizes are adjusted based on the
unadjusted interim variance estimate. The dotted lines show the
bias of the mean estimate using a samplesize reassessment rule is used
in order to maximize the negative or positive bias, respectively.

```{r}

tupd.mean.bias <- ggplot(gridsim,mapping=aes(x=delta,y=mean.bias)) + geom_path(lty=2)+
  geom_line(y=0,colour='gray') + geom_path(aes(y=m.bias),lty=3,col='red') + geom_path(aes(y=m.bias.n),lty=3,col='red') +  geom_path(aes(y=uc.mean.bias),lty=1) +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw()  + xlab(expression(delta)) + ylab('bias') +  ylim(-.2,.2)

print(tupd.mean.bias)

```


Next we look at variance estimates. 

```r

tupd.var.bias <- ggplot(gridsim,mapping=aes(x=delta,y=variance.bias)) +
    geom_line(y=0,colour='gray') + geom_path(aes(y=bound),color="red") +
  geom_path(lty=2) + geom_path(aes(y=uc.variance.bias),lty=1) +#geom_point(shape='V') + geom_point(aes(y=mean.bias),shape='M') +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-0.4,0.1) + xlab(expression(delta)) + ylab('bias')
print(tupd.var.bias)

```

In each simulated trial we estimated the standard error of the mean,
and compute the standard error of a fixed sample trial with the
same final sample size. Finally, we compute the standard deviation of
the mean estimate across all monte carlo samples, which is an unbiased
estimate of the true standard error of the mean. Figure ? shows the
results for designs that use the corrected interim variance
estimate to reassess the sample size; Figure ? shows the results for
designs that use the unadjusted interim variance estimate. Solid lines
show the standard deviation of mean estimates across simulated trials,
dashed lines show the average standard error estimates across
simulated trials, dotted lines show the average of the true standard
errors of corresponding fixed sample designs. The true standard error of the mean
of an adaptive design with blinded sample size reassessment is smaller
than that of corresponding fixed sample design. The bias of the
estimate of the standard error is close to zero around the null
hypothesis.  This holds both for designs using the adjusted and
unadjusted interim variance estimates to reassess the sample size. It
explains why  there is practically no inflation of the coverage
probability of confidence intervals near the null hypotheses, even
though the variance estimate has a considerable negative bias.

An explanation is
given in the following figure where we plot the standard error of the
mean, averaged across all simulated trials; the standard error of the
mean  of a fixed sample desig the standard deviation of
the mean estimate across all monte-carlo samples (i.e. the true
standard error of the mean)  of a given parameter setting
(dashed line); the Conditional on the second stage sample
size the estimate of the standard error is smaller than the standard
error of a fixed sample design with equal sample size. However the true
variance of mean - as estimated from all monte carlo samples - is 
estim mean estimate across all simulation 
runs. This shows that the standard error of a fixed sample design with
average sample size is larger than the standard error of the adaptive
design.  



```{r}

tupd.vm.bias <- ggplot(gridsim,mapping=aes(x=delta,y=vm)) +geom_line(lty=1) + geom_path(aes(y=ev),lty=2) + geom_path(aes(y=exv),lty=3) + 
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + xlab(expression(delta)) + ylab('Variance of the mean estimate')
print(tupd.vm.bias)

tupd.uc.vm.bias <- ggplot(gridsim,mapping=aes(x=delta,y=uc.vm)) +geom_line(lty=1) + geom_path(aes(y=uc.ev),lty=2) + geom_path(aes(y=uc.exv),lty=3) + 
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + xlab(expression(delta)) + ylab('Variance of the mean estimate')
print(tupd.uc.vm.bias)

```


```
lverbose = function(variable,value){
    if(variable == "s"){
        sapply(value,function(val) substitute(paste("s = ",foo,", ",n[1]," = ",n1,sep=''),list(foo=val,n1=ceiling(1/2*zss(val,1,.025,.2)))))
    } else {
        label_parsed(variable,label_both(variable,value))
    }
}

tupd <- ggplot(gridsim,mapping=aes(x=delta,y=.025-upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + #geom_point(shape='u') +
    geom_path(aes(y=.025-lower.prob),lty=3) + #geom_point(aes(y=.025-lower.prob),shape='l') +
    geom_path(aes(y=.05-total.prob)) + #geom_point(aes(y=.05-total.prob),shape='t') +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-.025,.01) + xlab(expression(delta)) + ylab('actual - nominal coverage')
print(tupd)


tupd.uc <- ggplot(gridsim,mapping=aes(x=delta,y=.025-uc.upper.prob)) +
    geom_line(y=0,colour='gray') +
    geom_path(lty=2) + # geom_point(shape='u') +
    geom_path(aes(y=.025-uc.lower.prob),lty=3) + #geom_point(aes(y=.025-uc.lower.prob),shape='l') +
    geom_path(aes(y=.05-uc.total.prob)) + #geom_point(aes(y=.05-uc.total.prob),shape='t') +
    facet_grid(sigma~s,labeller=lverbose) + theme_bw()+  ylim(-.025,.01) + xlab(expression(delta)) + ylab('actual -nominal coverage')
print(tupd.uc)



tupd.se.bias <- ggplot(gridsim,mapping=aes(x=delta,y=se.bias)) +
    geom_line(y=0,colour='gray') + geom_path(aes(y=bound),color="red") +
  geom_path(lty=1) + geom_path(aes(y=uc.se.bias),lty=2) +#geom_point(shape='V') + geom_point(aes(y=mean.bias),shape='M') +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw() + ylim(-0.2,0.2) + xlab(expression(delta)) + ylab('bias')
print(tupd.se.bias)

tupd.mean.bias <- ggplot(gridsim,mapping=aes(x=delta,y=mean.bias)) + geom_path(lty=1)+
  geom_line(y=0,colour='gray') + geom_path(aes(y=m.bias),lty=3,col='red') +  geom_path(aes(y=uc.mean.bias),lty=2) +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw()  + xlab(expression(delta)) + ylab('bias') # +  ylim(-.2,.2)
print(tupd.mean.bias)



tupd.mean.bias.sem <- ggplot(gridsim,mapping=aes(x=delta,y=mean.bias/(sigma/sqrt(n1)))) +
  geom_line(y=0,colour='gray') +  
  geom_path(lty=1) + geom_path(aes(y=uc.mean.bias/(sigma/sqrt(n1))),lty=2) +#geom_point(shape='V') +geom_point(aes(y=uc.mean.bias),shape='M') +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw() + xlab(expression(delta)) + ylab('bias')
print(tupd.mean.bias.sem)

tupd.ass <- ggplot(gridsim,mapping=aes(x=delta,y=(mean.m1+n1))) +
  geom_line(y=0,colour='gray') +  
  geom_path(lty=1) + geom_path(aes(y=uc.mean.m1+n1),lty=2) +#geom_point(shape='V') +geom_point(aes(y=uc.mean.bias),shape='M') +
  facet_grid(sigma~s,labeller=lverbose) + theme_bw() + xlab(expression(delta)) + ylab('average sample size')
print(tupd.ass)



pdf('coverage.pdf',9,8)
print(tupd)
dev.off()

pdf('uc_coverage.pdf',9,8)
print(tupd.uc)
dev.off()


pdf('mean_bias.pdf',9,8)
print(tupd.mean.bias)
dev.off()

pdf('mean_bias_sem_scaled.pdf',9,8)
print(tupd.mean.bias.sem)
dev.off()

pdf('sd_bias.pdf',9,8)
print(tupd.sd.bias)
dev.off()

pdf('asn.pdf',9,8)
print(tupd.ass)
dev.off()

```


Figures \ref{fig:bias} and \ref{fig:sd} show the (mean) bias of the
final estimates of the mean and standard deviation based on the total
sample. The estimates are biased for very small first stage sample
sizes or very large variances $\sigma$. For both sample size
reassessment rules $n_2(\fos)$ and $n_2(S_{1,OS,\pdelta}^2)$ the bias
of the mean is zero under the null hypothesis and has the opposite
sign as the true effectsize, otherwise. For very large positive and
negative effect sizes the bias is close to zero again. The latter is
due to the fact that for large effects that blinded interim variance
estimates are heavily positively biased leading to very large second
stage sample sizes. As a consequence the overall estimates are
essentially equal to the (unbiased) second stage estimates and the
bias becomes negligible. The estimate of the variance is negatively
biased and the bias appears to be maximised under the null
hypothesis. For positive and negative effect sizes the bias approaches
0 again due to the fact that the overall estimate becomes essentially
equal to the (unbiased) second stage estimates.


In general we observe that the absolute bias of mean and variance is larger when using the adjusted interim variance estimate compared to the case where the unadjusted estimate is used for sample size reassessment.

Eventhough the variance estimate is negatively biased, this does not imply that the estimated standard error is biased as well [\XXX es ist wohl problematisch ueber den Bias des SE zu sprechen aus gleichen Gruenden wie ueber den Bias der SD \XXX]. This is due to the fact, that the actual standard error $\sqrt{Var(\bar \Delta)}$ is lower than $\sqrt{E(\sigma^2/n)}$ as well.

Under the null hypothesis ($\delta=0$) the confidence interval has correct coverage, even for small sample sizes [Ausnahme Figure 3 2. Zeile, 2. Spalte?]. On first sight this is surprising, because the variance estimate is negatively biased, while the mean is unbiased.

The coverage probabilities of the confidence intervals are shown in Figures \ref{fig:uc.coverage} and \ref{fig:coverage}. Note that the coverage probability for the upper confidence bound at a certain true effect size $\delta$ is the same as the coverage probability for the lower confidence bound at $-\delta$. For positive $\delta$ the lower confidence bound is conservative (with coverage probability larger than 0.975) while the upper bound is anti-conservative. Over a large range of $\delta$  the two-sided coverage probability (which is given by one minus the sum of the non-coverage probabilities of the lower and upper bounds) is not controlled.


